from top2vec import Top2Vec
# import Doc2Vec
# 
# documents = ["Corneal.PMMA.Polymethylmethacrylate.corneal.polymethylmethacrylate","cornea.corneal.corneas","Hg.acetazolamide.elevation","cataract.corneal","cornea.corneal.keratoconus","conjunctivitis","Conjunctivitis.Corneal.Keratitis.corneal.epithelial.epithelium.filaments.fluorescein.keratitis.photophobia.rash.rose Bengal.young adult.young adults","Huntington's disease","protein - protein","plant","scale","G-protein coupled receptors.G-protein coupled receptors.Human.Ligands.environment.human.membrane.signal transduction"    ]
# newDocs = ["To examine the effect of adding artificial tears for one minute to normal eyes and eyes that have undergone keratoplasty by using computer-assisted videokeratography. We prospectively analyzed 24 normal corneas (24 patients) chosen by random number table (reproducibility section [10 eyes], tear section [14 eyes]) as well as 14 postkeratoplasty corneas. The parametric descriptors analyzed statistically included the simulated keratometry value, surface asymmetry index, surface regularity index, power at the vertex normal, and the pupil offsets from the visual axis. Vectoral analysis was used to calculate the difference in power and axis between the simulated keratometry values before and after tear instillation. In normal eyes, tear instillation increased the surface asymmetry index (0.28 +/- 0.34, P = .01), changed the simulated keratometry value (0.23 diopter by 27.8 degrees), and power (0.79 +/- 0.82 diopters, P = .004) and location (0.91 +/- 0.76 mm) of the steepest point of the cornea changed. In postkeratoplasty eyes, the surface regularity index decreased (0.49 +/- 0.80, P = .04), surface asymmetry index decreased (0.37 +/- 1.03, P = .21), mean simulated keratometry value changed (1.04 diopters by 1.01 degrees), and power (0.70 +/- 2.34 diopters, P = .28) and location (1.04 +/- 1.17 mm) of the steepest point of the cornea changed. Changes were greater than the variability of the surface regularity index (0.07 +/- 0.05), surface asymmetry index (0.04 +/- 0.03), simulated keratometry value power (0.08 +/- 0.06 diopter), and axis (4.6 +/- 5 degrees). The addition of artificial tears to normal or regular and symmetric eyes that have undergone keratoplasty worsened symmetry and changed the power and location of the steepest point. However, the addition of artificial tears to irregular eyes that have undergone penetrating keratoplasty created a more regular and symmetric surface and significantly altered the simulated keratometry values. We recommend that corneal topography be performed before the application of artificial tears.", "To determine the corneal topographic appearance in a pair of monozygotic twins and family members of the twins because one of the twins had keratoconus and the other appeared normal by clinical examination. Clinical examination and videokeratography (Topographic Modeling System, Tomey) of the patient, his monozygotic twin brother, an older brother, and his parents were performed. The I-S values (difference in the average dioptric powers of symmetrical points between the inferior and superior cornea) were calculated. The patient, a 28-year-old man, had clinical keratoconus confirmed with videokeratography. Clinical examination of family members including a twin brother, an older brother and both parents revealed no corneal abnormalities. Videokeratography of the clinically normal twin brother showed inferior steepening with progression over time. The I-S value of the clinically normal brother was 1.36 (right eye) (greater than 2.00 SD of normal controls), which progressed to 1.69 (right eye), 1.32 (left eye) 5 months later and to 1.87 (right eye), 1.43 diopters (D) (left eye) 14 months later. Minimal asymmetric inferior steepening was noted in an older brother who had an I-S value of 0.81 (right eye), 1.27 (left eye). The mother appeared topographically normal. This study lends support to the existence of subclinical keratoconus detectable by videokeratography only."]#,"The quantities of data obtained by the new high-throughput technologies, such as microarrays or ChIP-Chip arrays, and the large-scale OMICS-approaches, such as genomics, proteomics and transcriptomics, are becoming vast. Sequencing technologies become cheaper and easier to use and, thus, large-scale evolutionary studies towards the origins of life for all species and their evolution becomes more and more challenging. Databases holding information about how data are related and how they are hierarchically organized expand rapidly. Clustering analysis is becoming more and more difficult to be applied on very large amounts of data since the results of these algorithms cannot be efficiently visualized. Most of the available visualization tools that are able to represent such hierarchies, project data in 2D and are lacking often the necessary user friendliness and interactivity. For example, the current phylogenetic tree visualization tools are not able to display easy to understand large scale trees with more than a few thousand nodes. In this study, we review tools that are currently available for the visualization of biological trees and analysis, mainly developed during the last decade. We describe the uniform and standard computer readable formats to represent tree hierarchies and we comment on the functionality and the limitations of these tools. We also discuss on how these tools can be developed further and should become integrated with various data sources. Here we focus on freely available software that offers to the users various tree-representation methodologies for biological data analysis.","Elucidating the content of a DNA sequence is critical to deeper understand and decode the genetic information for any biological system. As next generation sequencing (NGS) techniques have become cheaper and more advanced in throughput over time, great innovations and breakthrough conclusions have been generated in various biological areas. Few of these areas, which get shaped by the new technological advances, involve evolution of species, microbial mapping, population genetics, genome-wide association studies (GWAs), comparative genomics, variant analysis, gene expression, gene regulation, epigenetics and personalized medicine. While NGS techniques stand as key players in modern biological research, the analysis and the interpretation of the vast amount of data that gets produced is a not an easy or a trivial task and still remains a great challenge in the field of bioinformatics. Therefore, efficient tools to cope with information overload, tackle the high complexity and provide meaningful visualizations to make the knowledge extraction easier are essential. In this article, we briefly refer to the sequencing methodologies and the available equipment to serve these analyses and we describe the data formats of the files which get produced by them. We conclude with a thorough review of tools developed to efficiently store, analyze and visualize such data with emphasis in structural variation analysis and comparative genomics. We finally comment on their functionality, strengths and weaknesses and we discuss how future applications could further develop in this field.","Text mining and data integration methods are gaining ground in the field of health sciences due to the exponential growth of bio-medical literature and information stored in biological databases. While such methods mostly try to extract bioentity associations from PubMed, very few of them are dedicated in mining other types of repositories such as chemical databases. Herein, we apply a text mining approach on the DrugBank database in order to explore drug associations based on the DrugBank "Description", "Indication", "Pharmacodynamics" and "Mechanism of Action" text fields. We apply Name Entity Recognition (NER) techniques on these fields to identify chemicals, proteins, genes, pathways, diseases, and we utilize the TextQuest algorithm to find additional biologically significant words. Using a plethora of similarity and partitional clustering techniques, we group the DrugBank records based on their common terms and investigate possible scenarios why these records are clustered together. Different views such as clustered chemicals based on their textual information, tag clouds consisting of Significant Terms along with the terms that were used for clustering are delivered to the user through a user-friendly web interface. DrugQuest is a text mining tool for knowledge discovery: it is designed to cluster DrugBank records based on text attributes in order to find new associations between drugs. The service is freely available at http://bioinformatics.med.uoc.gr/drugquest ."]
# 
# model = Top2Vec(documents=newDocs, speed="fast-learn", workers=8)

#model = Top2Vec(documents, embedding_model='universal-sentence-encoder-large')
#model.get_num_topics()


#Working dataset
from sklearn.datasets import fetch_20newsgroups

newsgroups = fetch_20newsgroups(subset='all', remove=('headers', 'footers', 'quotes'))

model = Top2Vec(documents=newsgroups.data[:500], speed="fast-learn", workers=8)
model.get_num_topics()
# print(type(newsgroups.data))
# 
# #end of working
# copiedTexts = []
# for i in range(10):
#   text = newsgroups.data[i]
#   copiedTexts.append(text)
#   
# print(type(newDocs))
# model = Top2Vec(documents=newDocs, speed="fast-learn", workers=8)


